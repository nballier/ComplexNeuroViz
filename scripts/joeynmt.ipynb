{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"joeynmt.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOSCK/+TMTxvYGV6cUzOWCm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KWvcCNA_wwoU"},"source":["## Import data "]},{"cell_type":"code","metadata":{"id":"MreREzM1v5N3"},"source":["# connecting to google collab\n","# https://joeynmt.readthedocs.io/en/latest/py-modindex.html\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m8b2vjUixbot"},"source":["## Pre-processing"]},{"cell_type":"code","metadata":{"id":"W_cO2NlXxdZY"},"source":["import os\n","source_langage = 'fr'\n","target_langage = 'en'\n","lc = False # lowercase if True\n","seed = 42 # random seed for shuffling\n","\n","\n","os.environ['src'] = source_langage\n","os.environ['tgt'] = target_langage\n","\n","os.environ['gdrive_path'] = '/content/drive/MyDrive/Data' # path of files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5l0o2m5kxkRg"},"source":["source_file =  '/content/drive/MyDrive/Data/file.src'\n","target_file = '/content/drive/MyDrive/Data/file.tgt'\n","\n","! wc -l $source_file\n","! wc -l $target_file \n","\n","!grep '^$' $source_file | wc -l # blank line in source\n","!grep '^$' $target_file | wc -l # blank line in target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_h_MczIzyC9G"},"source":["# remove blank lines\n","! mkdir  \"$gdrive_path/clean\"\n","\n","f1 = open('/content/drive/MyDrive/Data/clean/file.src','w')\n","f2 = open('/content/drive/MyDrive/Data/clean/file.tgt','w')\n","\n","with open(source_file) as fr, open(target_file) as en:\n","  for source, target in zip(fr.readlines(), en.readlines()):\n","    if source != '\\n' and target != '\\n':\n","      f1.write(f'{source}')\n","      f2.write(f'{target}')\n","f1.close()\n","f2.close()\n","source_file = \"/content/drive/MyDrive/Data/clean/file.src\"\n","target_file = \"/content/drive/MyDrive/Data/clean/file.tgt\"\n","\n","! wc -l $source_file\n","! wc -l $target_file\n","\n","!grep '^$' $source_file | wc -l # blank line in source\n","!grep '^$' $target_file | wc -l # blank line in target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uv-L7nPzyKzP"},"source":["## Tokenization"]},{"cell_type":"code","metadata":{"id":"tWR43GiQyMYu"},"source":["! mkdir \"$gdrive_path/token\"\n","token_directory = \"/content/drive/MyDrive/Data/token/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bQUKmvScyQrL"},"source":["### Sacremoses"]},{"cell_type":"code","metadata":{"id":"muxeuiOUySpk"},"source":["! pip install sacremoses\n","\n","tok_source_file = token_directory+\"file.src\"\n","tok_target_file = token_directory+\"file.tgt\"\n","\n","# Tokenize the source\n","! sacremoses -l $source_langage tokenize < $source_file > $tok_source_file\n","# Tokenize the target\n","! sacremoses -l $target_langage tokenize < $target_file > $tok_target_file\n","\n","! head -n 3 $source_file*\n","! head -n 3 $target_file*\n","\n","source_file = tok_source_file\n","target_file = tok_target_file"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aDNwXdckygfk"},"source":["### nltk"]},{"cell_type":"code","metadata":{"id":"qt2tFDqvyhoH"},"source":["import numpy as np\n","import nltk\n","nltk.download('punkt')\n","\n","tok_source_file = token_directory+\"file.src\"\n","tok_target_file = token_directory+\"file.tgt\"\n","\n","\n","def tokenization(file_name, name):\n","\toutput = open(name, 'w')\n","\twith open(file_name) as f:\n","\t\tfor line in f.readlines():\n","\t\t\ttokens = nltk.word_tokenize(line)\n","\t\t\tfor word in tokens:\n","\t\t\t\toutput.write(f'{word} ') \n","\t\t\toutput.write('\\n')\n","\toutput.close()\n","\t\n","tokenization(source_file, tok_source_file)\n","tokenization(target_file,tok_target_file)\n","\n","! head -n 3 $tok_source_file*\n","! head -n 3 $tok_target_file*\n","\n","source_file =  tok_source_file\n","target_file = tok_target_file"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lrz7NVpTyohm"},"source":["## Create train, dev and test "]},{"cell_type":"code","metadata":{"id":"RctsQQL9yseK"},"source":["# Create trainset and testset\n","\n","import pandas as pd\n","\n","source_file =  '/content/drive/MyDrive/Data/token/file.src'\n","target_file = '/content/drive/MyDrive/Data/token/file.tgt'\n","max = 500_000 # Numbrer of lines for training\n","source = []\n","target = []\n","source_test = []   \n","target_test = []\n","skip_lines = [] # line numbers to skip to the source and the target\n","  \n","with open(source_file) as f:\n","  for i, line in enumerate(f):\n","    if i == max:\n","      break  \n","    # split 20% of training into test\n","    if i % 5 == 0:\n","      source_test.append(line.strip())\n","      skip_lines.append(i) \n","    else:\n","      source.append(line.strip())   \n","with open(target_file) as f:\n","  for i, line in enumerate(f):\n","    if i == max: \n","      break  \n","    if i not in skip_lines:    \n","      target.append(line.strip())\n","    else:\n","      target_test.append(line.strip())\n","\n","print(f'Number of lines of the corpus {max}, Number of lines of the test {len(skip_lines)}')\n","\n","df = pd.DataFrame(zip(source, target), columns= ['source_sentence', 'target_sentence'])\n","test = pd.DataFrame(zip(source_test, target_test), columns=['source_sentence', 'target_sentence'])\n","\n","print('train...')\n","print(df.head(3))\n","print('test...')  \n","print(test.head(3))  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SUsckfpny0wN"},"source":["# Drop duplicate translations\n","df_pp = df.drop_duplicates()\n","\n","# Drop confliting translations\n","df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n","df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n","\n","#shuffle the data to remove bias in dev set selection\n","df_pp = df_pp.sample(frac = 1, random_state = seed).reset_index(drop = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7OHx2M9Py1aa"},"source":["# split train and dev\n","\n","import csv\n","\n","! mkdir \"$gdrive_path/train\"\n","! mkdir \"$gdrive_path/test\"\n","! mkdir \"$gdrive_path/dev\"\n","\n","train_src = \"/content/drive/MyDrive/Data/train/train.src\"\n","train_tgt = \"/content/drive/MyDrive/Data/train/train.tgt\"\n","test_src = \"/content/drive/MyDrive/Data/test/test.src\"\n","test_tgt = \"/content/drive/MyDrive/Data/test/test.tgt\"\n","dev_src = \"/content/drive/MyDrive/Data/dev/dev.src\"\n","dev_tgt = \"/content/drive/MyDrive/Data/dev/dev.tgt\"\n","\n","num_dev_patterns = 50_000\n","\n","if lc: # lowercase\n","  df_pp['source_sentence'] = df_pp['source_sentence'].str.lower()\n","  df_pp['target_sentence'] = df_pp['target_sentence'].str.lower()\n","  test['source_sentence'] = test['source_sentence'].str.lower()\n","  test['target_sentence'] = test['target_sentence'].str.lower()\n","\n","dev = df_pp.tail(num_dev_patterns)\n","stripped = df_pp.drop(df_pp.tail(num_dev_patterns).index)\n","\n","with open(train_src, 'w') as src_file, open(train_tgt,'w') as trg_file:\n","  for index, row in stripped.iterrows():\n","    src_file.write(row['source_sentence']+'\\n')\n","    trg_file.write(row['target_sentence']+'\\n')\n","\n","with open(dev_src, 'w') as src_file, open(dev_tgt,'w') as trg_file:\n","  for index, row in dev.iterrows():\n","    src_file.write(row['source_sentence']+'\\n')\n","    trg_file.write(row['target_sentence']+'\\n')\n","\n","with open(test_src, 'w') as src_file, open(test_tgt,'w') as trg_file:\n","  for index, row in test.iterrows():\n","    src_file.write(row['source_sentence']+'\\n')\n","    trg_file.write(row['target_sentence']+'\\n')\n","  \n","! head /content/drive/MyDrive/Data/train/*\n","! head /content/drive/MyDrive/Data/dev/*\n","! head /content/drive/MyDrive/Data/test/*"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"86Tgc-Rty8fh"},"source":["## Install joeynmt"]},{"cell_type":"code","metadata":{"id":"SdyXyffay-ZK"},"source":["# Install joeyNMT\n","! git clone https://github.com/joeynmt/joeynmt.git\n","! cd joeynmt; pip3 install ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4FD23rGazFY2"},"source":["# change cuda version\n","!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FTmXEBk3zImT"},"source":["## BPE"]},{"cell_type":"markdown","metadata":{"id":"9l8GbcjzzKZE"},"source":["### subwordnmt"]},{"cell_type":"code","metadata":{"id":"YLWFCLf5zMz9"},"source":["# Usually, NMT would tokenize by words. However, using BPE boosts the performance\n","# Byte pair encoding: decrease size of memory(large vocabulary to represent word pretty well)\n","# subword NMT\n","!pip install subword-nmt\n","from os import path\n","os.environ['src'] = source_langage\n","os.environ['tgt'] = target_langage\n","! mkdir \"$gdrive_path/subwordnmt\"\n","vocab_src = \"/content/drive/MyDrive/Data/subwordnmt/vocab.src\"\n","vocab_tgt = \"/content/drive/MyDrive/Data/subwordnmt/vocab.tgt\"\n","\n","train_bpe_src = '/content/drive/MyDrive/Data/subwordnmt/train.src'\n","train_bpe_tgt = '/content/drive/MyDrive/Data/subwordnmt/train.tgt'\n","test_bpe_src = '/content/drive/MyDrive/Data/subwordnmt/test.src'\n","test_bpe_tgt = '/content/drive/MyDrive/Data/subwordnmt/test.tgt'\n","dev_bpe_src = '/content/drive/MyDrive/Data/subwordnmt/dev.src'\n","dev_bpe_tgt = '/content/drive/MyDrive/Data/subwordnmt/dev.tgt'\n","# Learn BPE on the training data\n","! subword-nmt learn-joint-bpe-and-vocab --input $train_src $train_tgt -s 32000 -o bpe.codes --write-vocabulary $vocab_src $vocab_tgt\n","\n","# Apply bpe on train, dev and test\n","! subword-nmt apply-bpe -c bpe.codes --vocabulary $vocab_src < $train_src > $train_bpe_src\n","! subword-nmt apply-bpe -c bpe.codes --vocabulary $vocab_tgt < $train_tgt > $train_bpe_tgt\n","\n","! subword-nmt apply-bpe -c bpe.codes --vocabulary $vocab_src < $dev_src > $dev_bpe_src\n","! subword-nmt apply-bpe -c bpe.codes --vocabulary $vocab_tgt < $dev_tgt > $dev_bpe_tgt\n","\n","! subword-nmt apply-bpe -c bpe.codes --vocabulary $vocab_src < $test_src > $test_bpe_src\n","! subword-nmt apply-bpe -c bpe.codes --vocabulary $vocab_tgt < $test_tgt > $test_bpe_tgt\n","\n","\n","# somme output\n","! echo \"BPE sentences source\"\n","! head -n 5 $train_bpe_src\n","! echo \"BPE sentences target\"\n","! head -n 5 $train_bpe_tgt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_QdW9ZJ5zQeB"},"source":["### sentencepiece"]},{"cell_type":"code","metadata":{"id":"8yZpF8t5zSHL"},"source":["! pip install sentencepiece\n","! sudo apt-get install cmake build-essential pkg-config libgoogle-perftools-dev\n","\n","!git clone https://github.com/google/sentencepiece.git ; cd sentencepiece ; mkdir build ; cd build; cmake .. ; make -j $(nproc) ; sudo make install ; sudo ldconfig -v"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rh1_vXRqzYaJ"},"source":["from os import path\n","os.environ['src'] = source_langage\n","os.environ['tgt'] = target_langage\n","! mkdir \"$gdrive_path/sentencepiece\"\n","vocab_src = \"/content/drive/MyDrive/Data/sentencepiece/vocab.src\"\n","vocab_tgt = \"/content/drive/MyDrive/Data/sentencepiece/vocab.tgt\"\n","\n","train_src = \"/content/drive/MyDrive/Data/train/train.src\"\n","train_tgt = \"/content/drive/MyDrive/Data/train/train.tgt\"\n","test_src = \"/content/drive/MyDrive/Data/test/test.src\"\n","test_tgt = \"/content/drive/MyDrive/Data/test/test.tgt\"\n","dev_src = \"/content/drive/MyDrive/Data/dev/dev.src\"\n","dev_tgt = \"/content/drive/MyDrive/Data/dev/dev.tgt\"\n","\n","\n","train_bpe_src = '/content/drive/MyDrive/Data/sentencepiece/train.src'\n","train_bpe_tgt = '/content/drive/MyDrive/Data/sentencepiece/train.tgt'\n","test_bpe_src = '/content/drive/MyDrive/Data/sentencepiece/test.src'\n","test_bpe_tgt = '/content/drive/MyDrive/Data/sentencepiece/test.tgt'\n","dev_bpe_src = '/content/drive/MyDrive/Data/sentencepiece/dev.src'\n","dev_bpe_tgt = '/content/drive/MyDrive/Data/sentencepiece/dev.tgt'\n","os.environ['data_path'] = path.join('joeynmt', 'data', source_langage + target_langage)\n","\n","# Train sentencepiece model in source file (src.model and src.vocab are generated)\n","! spm_train --input=$train_src  --model_prefix=src --vocab_size=32000\n","\n","# Train sentencepiece model in target file\n","! spm_train --input=$train_tgt --model_prefix=tgt --vocab_size=32000\n","\n","# Train model in source files\n","! spm_encode --model=src.model --output_format=sample_piece < $train_src > $train_bpe_src\n","! spm_encode --model=src.model --output_format=sample_piece < $dev_src > $dev_bpe_src\n","! spm_encode --model=src.model --output_format=sample_piece < $test_src > $test_bpe_src\n","\n","# Train model in target files\n","! spm_encode --model=src.model --output_format=sample_piece < $train_tgt > $train_bpe_tgt\n","! spm_encode --model=src.model --output_format=sample_piece < $dev_tgt > $dev_bpe_tgt\n","! spm_encode --model=src.model --output_format=sample_piece < $test_tgt > $test_bpe_tgt\n","\n","! cp src.vocab /content/drive/MyDrive/Data/sentencepiece/vocab.src\n","! cp tgt.vocab /content/drive/MyDrive/Data/sentencepiece/vocab.tgt\n","\n","# create the directory\n","! mkdir -p $data_path \n","! cp /content/drive/MyDrive/Data/sentencepiece/* $data_path\n","! ls $data_path\n","\n","\n","# Create the vocab for joeynmt\n","! sudo chmod 777 joeynmt/scripts/build_vocab.py\n","! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.src joeynmt/data/$src$tgt/train.tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n","! cp joeynmt/data/$src$tgt/vocab.txt drive/MyDrive/Data/sentencepiece/\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r1eWcIu9zoWb"},"source":["## Data is already cleaned"]},{"cell_type":"code","metadata":{"id":"3tZOJCyMzsFV"},"source":["#run only if data is already tokenize(bpe) and model already created\n","! mkdir -p joeynmt/data/$src$tgt\n","! cp drive/MyDrive/Data/sentencepiece/* joeynmt/data/$src$tgt\n","#! cp drive/MyDrive/Data/subwordnmt/* joeynmt/data/$src$tgt\n","\n","!mkdir -p joeynmt/models/${src}${tgt}_transformer/\n","#!cp -r drive/MyDrive/joeynmt/models/${src}${tgt}_transformer/* joeynmt/models/${src}${tgt}_transformer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MTC0xSuTz2SM"},"source":["## Config yaml file"]},{"cell_type":"code","metadata":{"id":"qUkpqWdaz4d1"},"source":["# create joeyNMT model\n","\n","name = '%s%s' % (source_langage, target_langage)\n","gdrive_path = os.environ['gdrive_path']\n","\n","# create the config \n","# Rename files\n","! mv joeynmt/data/fren/train.src joeynmt/data/fren/train.fr\n","! mv joeynmt/data/fren/train.tgt joeynmt/data/fren/train.en\n","! mv joeynmt/data/fren/dev.src joeynmt/data/fren/dev.fr\n","! mv joeynmt/data/fren/dev.tgt joeynmt/data/fren/dev.en\n","! mv joeynmt/data/fren/test.src joeynmt/data/fren/test.fr\n","! mv joeynmt/data/fren/test.tgt joeynmt/data/fren/test.en\n","config = \"\"\"\n","name : \"{name}_transformer\"\n","\n","data: \n","  src:  \"{source_langage}\"\n","  trg: \"{target_langage}\"\n","  train: \"data/{name}/train\"\n","  dev: \"data/{name}/dev\"\n","  test: \"data/{name}/test\"\n","  level: \"bpe\"\n","  lowercase: False\n","  max_sent_length: 100\n","  src_vocab: \"data/{name}/vocab.txt\"\n","  tgr_vocab: \"data/{name}/vocab.txt\"\n","\n","testing:\n","  beam_size: 5\n","  alpha: 1.0\n","\n","training:\n","  #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load pre-trained model from this checkpoint\n","  random_seed: 42\n","  optimizer: \"adam\"\n","  normalization: \"tokens\"\n","  adam_betas: [0.9, 0.999]\n","  scheduling: \"plateau\"\n","  patience: 5\n","  learning_rate_factor: 0.5\n","  learning_rate_warmup: 1000\n","  decrease_factor: 0.7\n","  loss: \"crossentropy\"\n","  learning_rate: 0.0003\n","  learning_rate_min: 0.00000001\n","  weight_decay: 0.0\n","  label_smoothing: 0.1\n","  batch_size: 4096\n","  batch_type: \"token\"\n","  eval_batch_size: 3600\n","  eval_batch_type: \"token\"\n","  batch_multiplier: 1\n","  early_stopping_metric: \"ppl\"\n","  epochs: 5\n","  validation_freq: 30\n","  logging_freq: 5\n","  eval_metric: \"bleu\"\n","  model_dir: \"models/{name}_transformer\"\n","  overwrite: True\n","  shuffle: True\n","  use_cuda: True # to use GPU\n","  max_output_length: 100\n","  print_valid_sents: [0,1,2,3]\n","  keep_last_ckpts: 3\n","\n","model: \n","  initializer: \"xavier\"\n","  bias_initializer: \"zeros\"\n","  init_gain: 1.0\n","  embed_initializer: \"xavier\"\n","  embed_init_gain: 1.0\n","  #tied_embeddings: True\n","  tied_softmax: True\n","  encoder:\n","    type: \"transformer\"\n","    num_layers: 6\n","    num_heads: 4\n","    embeddings:\n","      embedding_dim: 256\n","      scale: True\n","      dropout: 0.2\n","    hidden_size: 256\n","    ff_size: 1024\n","    dopout: 0.3\n","  decoder:\n","    type: \"transformer\"\n","    num_layers: 6\n","    num_heads: 4\n","    embeddings:\n","      embedding_dim: 256\n","      scale: True\n","      dropout: 0.2\n","    hidden_size: 256\n","    ff_size: 1024\n","    dopout: 0.3\n","\n","\"\"\".format(name=name, gdrive_path=os.environ['gdrive_path'], source_langage=source_langage, target_langage=target_langage)\n","with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n","  f.write(config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EUkFMfob0GqU"},"source":["# save models in the drive           \n","! mkdir -p drive/MyDrive/joeynmt/models/${src}${tgt}_transformer/ \n","!cp -r joeynmt/models/${src}${tgt}_transformer/* drive/MyDrive/joeynmt/models/${src}${tgt}_transformer/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GpdPTfcY0XwA"},"source":["## Precision and validation"]},{"cell_type":"code","metadata":{"id":"Bf6t3aVr0adv"},"source":["# Validations\n","! cat drive/MyDrive/joeynmt/models/${src}${tgt}_transformer/validations.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZUwsyLoe0ffU"},"source":["%load_ext tensorboard\n","%tensorboard --logdir joeynmt/models/fren_transformer//tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fDmK6Ho80QYN"},"source":["## Translate"]},{"cell_type":"code","metadata":{"id":"tItjASV40SEu"},"source":["# translate a sentence\n","!cd joeynmt/ ; python3 -m joeynmt translate models/fren_transformer/config.yaml "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Fgn7TBR1zt6"},"source":["!cd joeynmt/ ; python3 -m joeynmt translate models/fren_transformer/config.yaml < data/fren/test.fr"],"execution_count":null,"outputs":[]}]}